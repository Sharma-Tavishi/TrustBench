{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4992443e-2a9a-4a05-b401-ec6162eb6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a86d0c55-bad7-4d6b-94c9-2fde7c34213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET= 'openlifescienceai/medqa'\n",
    "DEFAULT_SUBSET = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99b55221-52a8-40c8-a3bf-f04c82f013c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(DATASET, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f64d2ab9-0d35-4b9f-80db-c231b0f106ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'data', 'subject_name'],\n",
       "    num_rows: 1273\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26febf94-8a21-4bab-a1b3-d20e5d3f8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(ds)))\n",
    "random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ceca4368-2f44-413c-8e9a-7a4c33b987bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=DEFAULT_SUBSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9218899-5e69-44d6-aad9-47515e68cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = indices[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "133d9ee7-8103-405b-a782-d8022388ed75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ac797d2e-13fc-4bfa-9ad2-65169e0750d2',\n",
       " 'data': {'Correct Answer': 'Nadolol',\n",
       "  'Correct Option': 'C',\n",
       "  'Options': {'A': 'Endoscopic sclerotherapy',\n",
       "   'B': 'Metoprolol',\n",
       "   'C': 'Nadolol',\n",
       "   'D': 'Repeat endoscopy'},\n",
       "  'Question': \"A 56-year-old man recently diagnosed with cirrhosis secondary to alcohol use presents to the clinic for a follow up evaluation. He states that he has abstained from alcohol and attends a support group regularly. He has not taken any new medications or encountered any sick contacts. The patient's blood pressure is 110/70 mmHg, pulse is 65/min, and respirations are 15/min. His physical exam is grossly unremarkable. He has brought an gastroduodenoscopy report for review, which reveals that the patient has small esophageal varices with red spots. What is the next best step to prevent bleeding?\"},\n",
       " 'subject_name': ''}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fa6a5c9-ef2b-46ee-8073-977b90d43aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = ds[indices[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a434e1fc-010e-43e9-82a6-d4f1b3c60f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Correct Answer': 'Glutamate capsule',\n",
       " 'Correct Option': 'A',\n",
       " 'Options': {'A': 'Glutamate capsule',\n",
       "  'B': 'Toxin B',\n",
       "  'C': 'IgA protease',\n",
       "  'D': 'Sulfatides'},\n",
       " 'Question': 'A 42-year-old man is brought to the emergency department because of a 2-day history of dyspnea, chest pain, and hemoptysis that were preceded by 4 days of myalgia and a low-grade fever. He is currently visiting the United States as a tourist from Turkey, where he works at a factory that processes sheep wool and hide. An x-ray of the chest shows widening of the mediastinum. A sputum culture grows gram-positive rods in gray colonies that have irregular margins with wavy projections on microscopy. Which of the following virulence factors of the causal organism increased its survival in host tissue?'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "003898f8-5c94-4dfd-94d6-e2bec8f170c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = row['data'][\"Question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1aa80e33-2568-4031-8afd-701a318af36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = f\"truth-{i:04d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "275eea26-24ea-4670-812a-fb5e5b1eb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = row['data']['Options'][row['data'][\"Correct Option\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e42853-aafe-4678-82ff-d2915f7c89b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f1e0ee9-c81a-49cd-886b-405121594f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompts = []\n",
    "refs = []\n",
    "for i, idx in enumerate(indices):\n",
    "    row = ds[int(idx)]\n",
    "    q = row['data'][\"Question\"]\n",
    "    best = row['data']['Options'][row['data'][\"Correct Option\"]]\n",
    "    rid = f\"truth-{i:04d}\"\n",
    "    prompts.append({\n",
    "            \"id\": rid,\n",
    "            \"prompt\": q,\n",
    "            \"system\": \"You are a concise, truthful assistant. Answer briefly and accurately.\"\n",
    "        })\n",
    "    refs.append({\"id\": rid, \"references\": best, \"reference\": best[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db9ea3e5-8b14-4fa1-ac0b-f34060744ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2964"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8061044e-a321-4721-b54b-9dd04c009603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Trilochanapala, last king of the Kabul Shahi dynasty, ruled in the site of which modern country?',\n",
       " 'answer': ['Afghanistan']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba6125af-e0b9-49cc-95e5-d0eeef64231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': \"who is a British physicist  and also a  British government's chief scientific adviser?\",\n",
       " 'answer': ['Frederick Alexander']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[indices[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1732f12d-f9a9-460d-a65b-5d9629c72bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'truth-0000',\n",
       "  'references': ['Afghanistan'],\n",
       "  'reference': 'Afghanistan'},\n",
       " {'id': 'truth-0001', 'references': ['yes'], 'reference': 'yes'},\n",
       " {'id': 'truth-0002',\n",
       "  'references': ['Frederick Alexander'],\n",
       "  'reference': 'Frederick Alexander'},\n",
       " {'id': 'truth-0003',\n",
       "  'references': ['\"Orchard County\"'],\n",
       "  'reference': '\"Orchard County\"'},\n",
       " {'id': 'truth-0004',\n",
       "  'references': ['political satire black comedy film'],\n",
       "  'reference': 'political satire black comedy film'},\n",
       " {'id': 'truth-0005', 'references': ['six'], 'reference': 'six'},\n",
       " {'id': 'truth-0006',\n",
       "  'references': ['Woody Woodpecker'],\n",
       "  'reference': 'Woody Woodpecker'},\n",
       " {'id': 'truth-0007',\n",
       "  'references': ['Fredric John Warburg'],\n",
       "  'reference': 'Fredric John Warburg'},\n",
       " {'id': 'truth-0008',\n",
       "  'references': ['Talia Shire'],\n",
       "  'reference': 'Talia Shire'},\n",
       " {'id': 'truth-0009',\n",
       "  'references': ['Old World fossil representatives'],\n",
       "  'reference': 'Old World fossil representatives'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6917d7-f39b-4361-99ee-14f5e7fa86a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6bd372-c5cd-4016-9c8e-f2fc63423f17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SEED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_truthful_qa\u001b[39m(n: \u001b[38;5;28mint\u001b[39m = DEFAULT_SUBSET, \n\u001b[32m      2\u001b[39m                         split: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m                         seed: \u001b[38;5;28mint\u001b[39m = \u001b[43mSEED\u001b[49m) -> Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m      4\u001b[39m     random.seed(seed)\n\u001b[32m      5\u001b[39m     dataset= \u001b[33m\"\u001b[39m\u001b[33mtruthful_qa\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'SEED' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_truthful_qa(n: int = DEFAULT_SUBSET, \n",
    "                        split: str = \"validation\", \n",
    "                        seed: int = SEED) -> Tuple[str, str]:\n",
    "    random.seed(seed)\n",
    "    dataset= \"truthful_qa\"\n",
    "    ds = load_dataset(dataset)[split]\n",
    "    # Each row has 'question' and 'best_answer' (plus more fields); use best_answer as reference.\n",
    "    indices = list(range(len(ds)))\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:n]\n",
    "    prompts = []\n",
    "    refs = []\n",
    "    for i, idx in enumerate(indices):\n",
    "        row = ds[int(idx)]\n",
    "        q = row[\"question\"]\n",
    "        best = (row.get(\"best_answer\") or \"\").strip()\n",
    "        # TruthfulQA sometimes provides additional correct answers\n",
    "        extra = row.get(\"correct_answers\") or row.get(\"correct_answers_list\") or []\n",
    "        ref_list = []\n",
    "        if best:\n",
    "            ref_list.append(best)\n",
    "        # ensure strings & strip\n",
    "        for r in extra:\n",
    "            if isinstance(r, str):\n",
    "                r = r.strip()\n",
    "                if r and r not in ref_list:\n",
    "                    ref_list.append(r)\n",
    "        rid = f\"truth-{i:04d}\"\n",
    "        prompts.append({\n",
    "            \"id\": rid,\n",
    "            \"prompt\": q,\n",
    "            \"system\": \"You are a concise, truthful assistant. Answer briefly and accurately.\"\n",
    "        })\n",
    "        # Store both a list for robust scoring and a single field for back-compat\n",
    "        refs.append({\"id\": rid, \"references\": ref_list, \"reference\": best})\n",
    "    prompts_path = os.path.join(DATA_DIR, f\"{dataset}_subset.jsonl\")\n",
    "    refs_path = os.path.join(DATA_DIR, f\"{dataset}_refs.jsonl\")\n",
    "    write_jsonl(prompts_path, prompts)\n",
    "    write_jsonl(refs_path, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ddcd0-2f57-4a1d-a71e-42a98b264aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e65572-3f84-4785-8dd9-ffcf6c180b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
