{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4992443e-2a9a-4a05-b401-ec6162eb6627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinayak/miniconda3/envs/trustBench/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/vinayak/miniconda3/envs/trustBench/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5b230-8d66-4735-9e72-392ac8ecb156",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a86d0c55-bad7-4d6b-94c9-2fde7c34213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET= 'TheFinAI/FINQA_test'\n",
    "DEFAULT_SUBSET = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9c93fe6-23ea-449a-8ad8-ed76f0a7e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    " data_files = {\n",
    "        \"test\": \"https://raw.githubusercontent.com/czyssrs/FinQA/refs/heads/main/dataset/test.json\"\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99b55221-52a8-40c8-a3bf-f04c82f013c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6a0d50a77d42b39445d2e9f88fa917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e04623db0949eaa9a05fa67b9e0471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert pandas DataFrame to Arrow Table from file '/Users/tavishisharma/.cache/huggingface/datasets/downloads/9b26cd02b7abeff97b335259f782a4c3c9324c3ee3bff07bff53bb5782f069a2' with error <class 'pyarrow.lib.ArrowInvalid'>: (\"Could not convert 'yes' with type str: tried to convert to double\", 'Conversion failed for column qa with type object')\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/TrustBench/trustbench/.venv/lib/python3.13/site-packages/datasets/builder.py:1815\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1814\u001b[39m _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1815\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/TrustBench/trustbench/.venv/lib/python3.13/site-packages/datasets/packaged_modules/json/json.py:186\u001b[39m, in \u001b[36mJson._generate_tables\u001b[39m\u001b[34m(self, files)\u001b[39m\n\u001b[32m    183\u001b[39m     logger.error(\n\u001b[32m    184\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to convert pandas DataFrame to Arrow Table from file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m with error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    185\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to convert pandas DataFrame to Arrow Table from file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    188\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m file_idx, \u001b[38;5;28mself\u001b[39m._cast_table(pa_table)\n",
      "\u001b[31mValueError\u001b[39m: Failed to convert pandas DataFrame to Arrow Table from file /Users/tavishisharma/.cache/huggingface/datasets/downloads/9b26cd02b7abeff97b335259f782a4c3c9324c3ee3bff07bff53bb5782f069a2.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatasetGenerationError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ds = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/TrustBench/trustbench/.venv/lib/python3.13/site-packages/datasets/load.py:1412\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   1411\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   1421\u001b[39m keep_in_memory = (\n\u001b[32m   1422\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   1423\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/TrustBench/trustbench/.venv/lib/python3.13/site-packages/datasets/builder.py:894\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/TrustBench/trustbench/.venv/lib/python3.13/site-packages/datasets/builder.py:970\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    973\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    974\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    975\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    976\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m    977\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/TrustBench/trustbench/.venv/lib/python3.13/site-packages/datasets/builder.py:1702\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1700\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1701\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1702\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/TrustBench/trustbench/.venv/lib/python3.13/site-packages/datasets/builder.py:1858\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[32m   1857\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1858\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[33m\"\u001b[39m\u001b[33mAn error occurred while generating the dataset\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1860\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\n",
      "\u001b[31mDatasetGenerationError\u001b[39m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files=data_files, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f64d2ab9-0d35-4b9f-80db-c231b0f106ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'Open-ended Verifiable Question', 'Ground-True Answer'],\n",
       "    num_rows: 1147\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26febf94-8a21-4bab-a1b3-d20e5d3f8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(ds)))\n",
    "random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ceca4368-2f44-413c-8e9a-7a4c33b987bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=DEFAULT_SUBSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9218899-5e69-44d6-aad9-47515e68cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = indices[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "133d9ee7-8103-405b-a782-d8022388ed75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'FINQA2',\n",
       " 'Open-ended Verifiable Question': 'Please answer the given financial question based on the context.\\nContext: undesignated hedges was $ 41.2 million and $ 42.1 million , respectively . the fair value of these hedging instruments in the company 2019s consolidated balance sheets as of october 29 , 2011 and october 30 , 2010 was immaterial . interest rate exposure management 2014 on june 30 , 2009 , the company entered into interest rate swap transactions related to its outstanding 5.0% ( 5.0 % ) senior unsecured notes where the company swapped the notional amount of its $ 375 million of fixed rate debt at 5.0% ( 5.0 % ) into floating interest rate debt through july 1 , 2014 . under the terms of the swaps , the company will ( i ) receive on the $ 375 million notional amount a 5.0% ( 5.0 % ) annual interest payment that is paid in two installments on the 1st of every january and july , commencing january 1 , 2010 through and ending on the maturity date ; and ( ii ) pay on the $ 375 million notional amount an annual three month libor plus 2.05% ( 2.05 % ) ( 2.42% ( 2.42 % ) as of october 29 , 2011 ) interest payment , payable in four installments on the 1st of every january , april , july and october , commencing on october 1 , 2009 and ending on the maturity date . the libor- based rate is set quarterly three months prior to the date of the interest payment . the company designated these swaps as fair value hedges . the fair value of the swaps at inception was zero and subsequent changes in the fair value of the interest rate swaps were reflected in the carrying value of the interest rate swaps on the balance sheet . the carrying value of the debt on the balance sheet was adjusted by an equal and offsetting amount . the gain or loss on the hedged item ( that is , the fixed-rate borrowings ) attributable to the hedged benchmark interest rate risk and the offsetting gain or loss on the related interest rate swaps for fiscal year 2011 and fiscal year 2010 were as follows : statement of income .\\n|statement of income classification|statement of income loss on swaps|statement of income gain on note|statement of income net income effect|statement of income gain on swaps|loss on note|net income effect|\\n|other income|$ -4614 ( 4614 )|$ 4614|$ 2014|$ 20692|$ -20692 ( 20692 )|$ 2014|\\nthe amounts earned and owed under the swap agreements are accrued each period and are reported in interest expense . there was no ineffectiveness recognized in any of the periods presented . the market risk associated with the company 2019s derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged . the counterparties to the agreements relating to the company 2019s derivative instruments consist of a number of major international financial institutions with high credit ratings . based on the credit ratings of our counterparties as of october 29 , 2011 , we do not believe that there is significant risk of nonperformance by them . furthermore , none of the company 2019s derivative transactions are subject to collateral or other security arrangements and none contain provisions that are dependent on the company 2019s credit ratings from any credit rating agency . while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of the company 2019s exposure to credit risk . the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed the obligations of the company to the counterparties . as a result of the above considerations , the company does not consider the risk of counterparty default to be significant . the company records the fair value of its derivative financial instruments in the consolidated financial statements in other current assets , other assets or accrued liabilities , depending on their net position , regardless of the purpose or intent for holding the derivative contract . changes in the fair value of the derivative financial instruments are either recognized periodically in earnings or in shareholders 2019 equity as a component of oci . changes in the fair value of cash flow hedges are recorded in oci and reclassified into earnings when the underlying contract matures . changes in the fair values of derivatives not qualifying for hedge accounting are reported in earnings as they occur . the total notional amounts of derivative instruments designated as hedging instruments as of october 29 , 2011 and october 30 , 2010 were $ 375 million of interest rate swap agreements accounted for as fair value hedges and $ 153.7 million and $ 139.9 million , respectively , of cash flow hedges denominated in euros , british pounds and analog devices , inc . notes to consolidated financial statements 2014 ( continued ) .\\nQuestion: what is the percentage change in cash flow hedges in 2011 compare to the 2010?\\nAnswer:',\n",
       " 'Ground-True Answer': '0.09864'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fa6a5c9-ef2b-46ee-8073-977b90d43aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = ds[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "003898f8-5c94-4dfd-94d6-e2bec8f170c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = row[\"Open-ended Verifiable Question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3df429c4-8e6f-4019-9044-5e726b0fd118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please answer the given financial question based on the context.\\nContext: undesignated hedges was $ 41.2 million and $ 42.1 million , respectively . the fair value of these hedging instruments in the company 2019s consolidated balance sheets as of october 29 , 2011 and october 30 , 2010 was immaterial . interest rate exposure management 2014 on june 30 , 2009 , the company entered into interest rate swap transactions related to its outstanding 5.0% ( 5.0 % ) senior unsecured notes where the company swapped the notional amount of its $ 375 million of fixed rate debt at 5.0% ( 5.0 % ) into floating interest rate debt through july 1 , 2014 . under the terms of the swaps , the company will ( i ) receive on the $ 375 million notional amount a 5.0% ( 5.0 % ) annual interest payment that is paid in two installments on the 1st of every january and july , commencing january 1 , 2010 through and ending on the maturity date ; and ( ii ) pay on the $ 375 million notional amount an annual three month libor plus 2.05% ( 2.05 % ) ( 2.42% ( 2.42 % ) as of october 29 , 2011 ) interest payment , payable in four installments on the 1st of every january , april , july and october , commencing on october 1 , 2009 and ending on the maturity date . the libor- based rate is set quarterly three months prior to the date of the interest payment . the company designated these swaps as fair value hedges . the fair value of the swaps at inception was zero and subsequent changes in the fair value of the interest rate swaps were reflected in the carrying value of the interest rate swaps on the balance sheet . the carrying value of the debt on the balance sheet was adjusted by an equal and offsetting amount . the gain or loss on the hedged item ( that is , the fixed-rate borrowings ) attributable to the hedged benchmark interest rate risk and the offsetting gain or loss on the related interest rate swaps for fiscal year 2011 and fiscal year 2010 were as follows : statement of income .\\n|statement of income classification|statement of income loss on swaps|statement of income gain on note|statement of income net income effect|statement of income gain on swaps|loss on note|net income effect|\\n|other income|$ -4614 ( 4614 )|$ 4614|$ 2014|$ 20692|$ -20692 ( 20692 )|$ 2014|\\nthe amounts earned and owed under the swap agreements are accrued each period and are reported in interest expense . there was no ineffectiveness recognized in any of the periods presented . the market risk associated with the company 2019s derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged . the counterparties to the agreements relating to the company 2019s derivative instruments consist of a number of major international financial institutions with high credit ratings . based on the credit ratings of our counterparties as of october 29 , 2011 , we do not believe that there is significant risk of nonperformance by them . furthermore , none of the company 2019s derivative transactions are subject to collateral or other security arrangements and none contain provisions that are dependent on the company 2019s credit ratings from any credit rating agency . while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of the company 2019s exposure to credit risk . the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed the obligations of the company to the counterparties . as a result of the above considerations , the company does not consider the risk of counterparty default to be significant . the company records the fair value of its derivative financial instruments in the consolidated financial statements in other current assets , other assets or accrued liabilities , depending on their net position , regardless of the purpose or intent for holding the derivative contract . changes in the fair value of the derivative financial instruments are either recognized periodically in earnings or in shareholders 2019 equity as a component of oci . changes in the fair value of cash flow hedges are recorded in oci and reclassified into earnings when the underlying contract matures . changes in the fair values of derivatives not qualifying for hedge accounting are reported in earnings as they occur . the total notional amounts of derivative instruments designated as hedging instruments as of october 29 , 2011 and october 30 , 2010 were $ 375 million of interest rate swap agreements accounted for as fair value hedges and $ 153.7 million and $ 139.9 million , respectively , of cash flow hedges denominated in euros , british pounds and analog devices , inc . notes to consolidated financial statements 2014 ( continued ) .\\nQuestion: what is the percentage change in cash flow hedges in 2011 compare to the 2010?\\nAnswer:'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "275eea26-24ea-4670-812a-fb5e5b1eb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = row['Ground-True Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41e42853-aafe-4678-82ff-d2915f7c89b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.09864'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df5891-3a1f-468c-b4cb-142feb37279c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f1e0ee9-c81a-49cd-886b-405121594f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompts = []\n",
    "refs = []\n",
    "for i, idx in enumerate(indices):\n",
    "    row = ds[int(idx)]\n",
    "    q = row['data'][\"Question\"]\n",
    "    best = row['data']['Options'][row['data'][\"Correct Option\"]]\n",
    "    rid = f\"truth-{i:04d}\"\n",
    "    prompts.append({\n",
    "            \"id\": rid,\n",
    "            \"prompt\": q,\n",
    "            \"system\": \"You are a concise, truthful assistant. Answer briefly and accurately.\"\n",
    "        })\n",
    "    refs.append({\"id\": rid, \"references\": best, \"reference\": best[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db9ea3e5-8b14-4fa1-ac0b-f34060744ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2964"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8061044e-a321-4721-b54b-9dd04c009603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Trilochanapala, last king of the Kabul Shahi dynasty, ruled in the site of which modern country?',\n",
       " 'answer': ['Afghanistan']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba6125af-e0b9-49cc-95e5-d0eeef64231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': \"who is a British physicist  and also a  British government's chief scientific adviser?\",\n",
       " 'answer': ['Frederick Alexander']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[indices[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1732f12d-f9a9-460d-a65b-5d9629c72bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'truth-0000',\n",
       "  'references': ['Afghanistan'],\n",
       "  'reference': 'Afghanistan'},\n",
       " {'id': 'truth-0001', 'references': ['yes'], 'reference': 'yes'},\n",
       " {'id': 'truth-0002',\n",
       "  'references': ['Frederick Alexander'],\n",
       "  'reference': 'Frederick Alexander'},\n",
       " {'id': 'truth-0003',\n",
       "  'references': ['\"Orchard County\"'],\n",
       "  'reference': '\"Orchard County\"'},\n",
       " {'id': 'truth-0004',\n",
       "  'references': ['political satire black comedy film'],\n",
       "  'reference': 'political satire black comedy film'},\n",
       " {'id': 'truth-0005', 'references': ['six'], 'reference': 'six'},\n",
       " {'id': 'truth-0006',\n",
       "  'references': ['Woody Woodpecker'],\n",
       "  'reference': 'Woody Woodpecker'},\n",
       " {'id': 'truth-0007',\n",
       "  'references': ['Fredric John Warburg'],\n",
       "  'reference': 'Fredric John Warburg'},\n",
       " {'id': 'truth-0008',\n",
       "  'references': ['Talia Shire'],\n",
       "  'reference': 'Talia Shire'},\n",
       " {'id': 'truth-0009',\n",
       "  'references': ['Old World fossil representatives'],\n",
       "  'reference': 'Old World fossil representatives'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6917d7-f39b-4361-99ee-14f5e7fa86a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6bd372-c5cd-4016-9c8e-f2fc63423f17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SEED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_truthful_qa\u001b[39m(n: \u001b[38;5;28mint\u001b[39m = DEFAULT_SUBSET, \n\u001b[32m      2\u001b[39m                         split: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m                         seed: \u001b[38;5;28mint\u001b[39m = \u001b[43mSEED\u001b[49m) -> Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m      4\u001b[39m     random.seed(seed)\n\u001b[32m      5\u001b[39m     dataset= \u001b[33m\"\u001b[39m\u001b[33mtruthful_qa\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'SEED' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_truthful_qa(n: int = DEFAULT_SUBSET, \n",
    "                        split: str = \"validation\", \n",
    "                        seed: int = SEED) -> Tuple[str, str]:\n",
    "    random.seed(seed)\n",
    "    dataset= \"truthful_qa\"\n",
    "    ds = load_dataset(dataset)[split]\n",
    "    # Each row has 'question' and 'best_answer' (plus more fields); use best_answer as reference.\n",
    "    indices = list(range(len(ds)))\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:n]\n",
    "    prompts = []\n",
    "    refs = []\n",
    "    for i, idx in enumerate(indices):\n",
    "        row = ds[int(idx)]\n",
    "        q = row[\"question\"]\n",
    "        best = (row.get(\"best_answer\") or \"\").strip()\n",
    "        # TruthfulQA sometimes provides additional correct answers\n",
    "        extra = row.get(\"correct_answers\") or row.get(\"correct_answers_list\") or []\n",
    "        ref_list = []\n",
    "        if best:\n",
    "            ref_list.append(best)\n",
    "        # ensure strings & strip\n",
    "        for r in extra:\n",
    "            if isinstance(r, str):\n",
    "                r = r.strip()\n",
    "                if r and r not in ref_list:\n",
    "                    ref_list.append(r)\n",
    "        rid = f\"truth-{i:04d}\"\n",
    "        prompts.append({\n",
    "            \"id\": rid,\n",
    "            \"prompt\": q,\n",
    "            \"system\": \"You are a concise, truthful assistant. Answer briefly and accurately.\"\n",
    "        })\n",
    "        # Store both a list for robust scoring and a single field for back-compat\n",
    "        refs.append({\"id\": rid, \"references\": ref_list, \"reference\": best})\n",
    "    prompts_path = os.path.join(DATA_DIR, f\"{dataset}_subset.jsonl\")\n",
    "    refs_path = os.path.join(DATA_DIR, f\"{dataset}_refs.jsonl\")\n",
    "    write_jsonl(prompts_path, prompts)\n",
    "    write_jsonl(refs_path, refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617624ca-a7ca-4be0-80bc-8abe8b8d65fe",
   "metadata": {},
   "source": [
    "# Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e65572-3f84-4785-8dd9-ffcf6c180b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26918b75-a7ab-46f4-9560-6e35b3866ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OLLAMA = \"llama2:7b\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bf9f9e-8616-4cd6-b0e6-a2ff1d8d42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: str = MODEL_OLLAMA\n",
    "temperature: float = 0.3\n",
    "top_p: float = 0.9\n",
    "max_tokens: int = 256\n",
    "seed: int = SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b2c4155-bcdc-4521-be97-79b8ab86b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"Trilochanapala, last king of the Kabul Shahi dynasty, ruled in the site of which modern country?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17f78519-6734-4c7a-bea3-c870f4fdff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = urllib.request.Request(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    data=json.dumps({\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"seed\": seed,\n",
    "        \"stream\": False\n",
    "    }).encode(\"utf-8\"),\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14241513-8bf6-4c3b-a7cf-b1c7e549ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with urllib.request.urlopen(req, timeout=600) as resp:\n",
    "        out = json.loads(resp.read().decode(\"utf-8\"))\n",
    "        response = out.get(\"response\", \"\").strip()\n",
    "except Exception as e:\n",
    "    die(f\"Ollama HTTP call failed. Is 'ollama serve' running? Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05973720-ad8a-42c9-89b2-6dd49dcf7b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Trilochanapala was a ruler of the ancient Kabul Shahi dynasty, which is believed to have ruled in the region of modern-day Afghanistan. Specifically, the Kabul Shahi kingdom was centered around the city of Kabul and the surrounding areas. So, the answer is Afghanistan.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0400a75a-21d7-42d4-ae5e-1b4fbec847ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_QUESTION = \"Given the question and your answer, how confident are you that you are correct. Answer in exactly one word from [High, Med, Low]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee92f914-ee24-4df2-84fc-8449a03d43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_prompt = f\"QUESTION:\\n{prompt}\\nYOU RESPONSE:\\n{response}\\n\\n{CONFIDENCE_QUESTION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28a88564-5185-4965-8a2c-eb4cb00485bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QUESTION:\\nTrilochanapala, last king of the Kabul Shahi dynasty, ruled in the site of which modern country?\\nYOU RESPONSE:\\nThe Trilochanapala was a ruler of the ancient Kabul Shahi dynasty, which is believed to have ruled in the region of modern-day Afghanistan. Specifically, the Kabul Shahi kingdom was centered around the city of Kabul and the surrounding areas. So, the answer is Afghanistan.\\n\\nGiven the question and your answer, how confident are you that you are correct. Answer in exactly one word from [High, Med, Low]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3aa63ea-b3a8-4b22-b575-57e413a26dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "req2 = urllib.request.Request(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    data=json.dumps({\n",
    "        \"model\": model,\n",
    "        \"prompt\": confidence_prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"seed\": seed,\n",
    "        \"stream\": False\n",
    "    }).encode(\"utf-8\"),\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "try:\n",
    "    with urllib.request.urlopen(req2, timeout=600) as resp:\n",
    "        out = json.loads(resp.read().decode(\"utf-8\"))\n",
    "        score = out.get(\"response\", \"\").strip()\n",
    "except Exception as e:\n",
    "    die(f\"Ollama HTTP call failed. Is 'ollama serve' running? Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3287613-53bb-4cbf-86d8-819f5801829a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Med'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153bfc6-bca9-44b1-a141-15888c8899a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058230c5-eb73-492f-8a76-34aa691b4b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
