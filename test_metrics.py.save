#!/usr/bin/env python3
"""
Smoke test for core metrics on a small sample (n=10).

What it does:
1) Prepare a 10-example TruthfulQA subset (data/truthfulqa_subset.jsonl, data/truthfulqa_refs.jsonl)
2) Generate model outputs via Ollama or MLX (results/outputs.jsonl)
3) Run reference-based metrics (EM/F1/ROUGE + optional BERTScore via --bertscore)
4) Run factual consistency (n-gram + NLI entailment; choose model with --nli-model)
5) Inject a citation-style output row to test citation integrity; run citation checks
6) Create a simple timeliness reference file for these ids and run timeliness

Sanity checks printed to stdout:
- Reference aggregate keys present and in range
- Factual consistency summary files exist and contain averages
- Citation checker flags the injected example
- Timeliness scores are within [0,1] and average makes sense

Usage:
  python test_metrics.py --backend ollama --subset 10 --nli-model facebook/bart-large-mnli
"""

import os, json, random, argparse, datetime as dt
from typing import Dict, List, Any, Tuple

# Local imports from your codebase
from trustbench import (
    prepare_truthfulqa_subset,
    run_generation,
    evaluate_reference,
    read_jsonl,
    write_jsonl,
    write_json,
    normalize,
    RESULTS_DIR,
    DATA_DIR,
)
from metrics.factual_consistency import evaluate_factual_consistency
from metrics.citation import analyze_citation_integrity
from metrics.timeliness import evaluate_time_aware

def ensure_dirs():
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(RESULTS_DIR, exist_ok=True)

def prepare_subset(n: int, seed: int = 42) -> Tuple[str, str]:
    # Uses your helper to create small subset + references
    return prepare_truthfulqa_subset(n=n, split="validation", seed=seed)

def maybe_generate(prompts_path: str, backend: str) -> str:
    out_path = os.path.join(RESULTS_DIR, "outputs.jsonl")
    # Always (re)generate for this smoke test to keep it honest
    return run_generation(prompts_path, backend)

def run_reference(outputs_path: str, refs_path: str, primary="rouge", do_bertscore=False) -> Dict[str, Any]:
    spath, summary = evaluate_reference(outputs_path, refs_path, primary, do_bertscore=do_bertscore)
    print("\n[REFERENCE] metrics_summary.json:")
    print(json.dumps(summary, indent=2))
    # sanity checks
    assert "aggregate" in summary and isinstance(summary["aggregate"], dict), "Missing aggregate in reference summary"
    for k in ["em","f1","rouge_l"]:
        assert k in summary["aggregate"], f"Missing {k} in reference aggregates"
        v = summary["aggregate"][k]
        assert 0.0 <= v <= 1.0, f"{k} out of range [0,1]: {v}"
    return summary

def run_factual(outputs_path: str, refs_path: str, nli_model: str) -> Dict[str, Any]:
    res = evaluate_factual_consistency(outputs_path, refs_path, nli_model=nli_model)
    print("\n[FACTUAL CONSISTENCY] factual_consistency_summary.json:")
    print(json.dumps(res, indent=2))
    # sanity checks
    assert "ngram" in res and "nli" in res, "Missing ngram or nli keys"
    assert "aggregate" in res["nli"], "Missing aggregate in NLI summary"
    for k in ["nli_entailment","nli_contradiction","nli_neutral"]:
        if k in res["nli"]["aggregate"] and res["nli"]["aggregate"][k] is not None:
            v = res["nli"]["aggregate"][k]
            assert 0.0 <= v <= 1.0, f"{k} out of range [0,1]: {v}"
    return res

def inject_citation_case(outputs_path: str):
    """
    Append one synthetic row with a citation-style completion to exercise the checker.
    This does NOT need a matching reference id; citation checker reads outputs only.
    """
    rows = read_jsonl(outputs_path)
    rows.append({
        "id": "citation-test",
        "prompt": "Name the capital of France",
        "completion": "Paris [1]\n\nReferences\n[1] https://en.wikipedia.org/wiki/Paris"
    })
    write_jsonl(outputs_path, rows)

def run_citation(outputs_path: str) -> Dict[str, Any]:
    spath, summary = analyze_citation_integrity(outputs_path)
    print("\n[CITATION] citation_summary.json:")
    print(json.dumps(summary, indent=2))
    # sanity checks
    assert "citation_rate" in summary, "Missing citation_rate"
    assert 0.0 <= summary["citation_rate"] <= 1.0, "citation_rate out of range"
    return summary

def build_simple_time_refs(refs_path: str) -> Dict[str, List[Dict[str, Any]]]:
    """
    Construct a trivial timeliness reference dict:
    - For each id, treat the primary reference as valid from 2000-01-01 to None (still valid).
    This is just to test the evaluation path; it won't penalize with decay.
    """
    refs = read_jsonl(refs_path)
    time_refs: Dict[str, List[Dict[str, Any]]] = {}
    for r in refs:
        rid = r["id"]
        # Choose first available gold as the canonical answer
        ans = None
        if r.get("references"):
            ans = r["references"][0]
        elif r.get("reference"):
            ans = r["reference"]
        if not ans:
            ans = ""  # fallback
        time_refs[rid] = [{
            "answer": ans,
            "valid_from": "2000-01-01",
            "valid_to": None
        }]
    # Save for inspection
    write_json(os.path.join(DATA_DIR, "time_refs.json"), time_refs)
    return time_refs

def run_timeliness(outputs_path: str, time_refs: Dict[str, List[Dict[str, Any]]], ref_date: str):
    outs = read_jsonl(outputs_path)
    items = [{"id": r["id"], "completion": r.get("completion","")} for r in outs if r.get("id","").startswith("truth-")]
    res = evaluate_time_aware(items, time_refs, ref_date=ref_date)
    print("\n[TIMELINESS] timeliness_summary.json:")
    print(json.dumps(res, indent=2))
    # sanity checks
    assert "avg_timeliness" in res, "Missing avg_timeliness"
    v = res["avg_timeliness"]
    assert 0.0 <= v <= 1.0, f"avg_timeliness out of range [0,1]: {v}"
    return res

def main():
    ap = argparse.ArgumentParser(description="Smoke test for TrustBench core metrics (10 examples).")
    ap.add_argument("--backend", choices=["ollama","mlx"], default="ollama", help="Generation backend")
    ap.add_argument("--subset", type=int, default=10, help="Subset size (default 10)")
    ap.add_argument("--nli-model", default="facebook/bart-large-mnli", help="NLI model for entailment")
    ap.add_argument("--bertscore", action="store_true", help="Also compute BERTScore in reference metrics")
    args = ap.parse_args()

    ensure_dirs()

    # 1) Prepare a small subset
    prompts_path, refs_path = prepare_subset(args.subset, seed=42)

    # 2) Generate outputs
    outputs_path = maybe_generate(prompts_path, backend=args.backend)

    # 3) Reference-based metrics
    ref_sum = run_reference(outputs_path, refs_path, primary="rouge", do_bertscore=args.bertscore)

    # 4) Factual consistency (n-gram + NLI entailment)
    fact_sum = run_factual(outputs_path, refs_path, nli_model=args.nli_model)

    # 5) Citation checker (inject one synthetic citation row to verify detection)
    inject_citation_case(outputs_path)
    cit_sum = run_citation(outputs_path)

    # 6) Timeliness (build trivial validity windows)
    today = dt.date.today().isoformat()
    trefs = build_simple_time_refs(refs_path)
    time_sum = run_timeliness(outputs_path, trefs, ref_date=today)

    print("\n===== SMOKE TEST COMPLETE =====")
    print("Files to inspect (short list):")
    for p in [
        "results/metrics_summary.json",
        "results/fconsistency_summary.json",
        "results/nli_summary.json",
        "results/citation_summary.json",
        "results/timeliness_summary.json",
    ]:
        print(f" - {p}")

if __name__ == "__main__":
    main()
